{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries from the langchain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing os and get some methods from it.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# os.environ['OPEN_API_KEY']= OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if openai_api_key:\n",
    "#     print(f\"Successfully loaded OPENAI_API_KEY: YOUR_OPENAI_API_KEY\")\n",
    "# else:\n",
    "#     print(\"Error: OPENAI_API_KEY not found in the .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature value: 0 means the model is very safe and it is not taking any bets. the more it tends to 1, it will be taking a lot more bets and risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the temperature parameter has important role like\n",
    "# its ranges from 0 to 1. The more tend to 1 means \n",
    "# that the different output we may have.\n",
    "llm=OpenAI(openai_api_key=openai_api_key, temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my question to be known\n",
    "text=\"What is the capital of Bangladesh?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of Bangladesh is Dhaka.\n"
     ]
    }
   ],
   "source": [
    "# print the result\n",
    "# llm.predict(text)\n",
    "print(llm.predict(text))\n",
    "\n",
    "#################3 this is end of using OPENAI-API-KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using flan-t5-large model by HuggingFace Token on the top of Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers python-dotenv        flan-t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# importing\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Access the Hugging Face API token\n",
    "hf_api_token = os.getenv(\"HF_HOME_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing HuggingFaceHub() from langchain and creating an object of using it\n",
    "from langchain import HuggingFaceHub\n",
    "llms_huggingfacehub=HuggingFaceHub(\n",
    "    huggingfacehub_api_token=hf_api_token, \n",
    "    repo_id='google/flan-t5-large', \n",
    "    model_kwargs={'temperature':0.6, 'max_length':64}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dhaka'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outout or prediction\n",
    "llms_huggingfacehub.predict(\"what is the capital of Bangladesh?\")\n",
    "# this is how we can use open source mode here we used \n",
    "# flan-t5-large by HuggingFace API Tokens on the top of Langchain.\n",
    "\n",
    "\n",
    "############ this is end of using HuggingFace API Tokens for using the open source model as 'flan-t5-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # these code is generated from chat.openai.com\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "# # Access the Hugging Face API token\n",
    "# hf_api_token = os.getenv(\"HF_HOME_TOKEN\")\n",
    "# if not hf_api_token:\n",
    "#     raise ValueError(\"Hugging Face API token not found in the .env file.\")\n",
    "\n",
    "# # Set the API token for Hugging Face authentication\n",
    "# os.environ[\"HF_HOME_TOKEN\"] = hf_api_token\n",
    "\n",
    "# # Replace 'flan-t5-large' with the actual model identifier\n",
    "# model_name = \"google/flan-t5-large\"\n",
    "\n",
    "# # Load the tokenizer and model  ########here the error occured #####\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# # Example input text\n",
    "# input_text = \"Translate this text to another language.\"\n",
    "\n",
    "# # Tokenize the input text\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# # Generate output from the model\n",
    "# output_ids = model.generate(input_ids)\n",
    "\n",
    "# # Decode the output tokens to get the translated text\n",
    "# output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# # Print the result\n",
    "# print(f\"Input Text: {input_text}\")\n",
    "# print(f\"Translated Text: {output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompt Templates:\n",
    "- When we call LLM model then the LLM model should know what kind of \n",
    "- input it is probably expecting from the client or from the end user \n",
    "- and what kind of output it should give back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PromptTemplate() class from langchain.prompts\n",
    "# and creating an object of the PromptTemplate class\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the object llms_huggingfacehub\n",
    "# from langchain import HuggingFaceHub\n",
    "# llms_huggingfacehub=HuggingFaceHub(\n",
    "#     huggingfacehub_api_token=hf_api_token, \n",
    "#     repo_id='google/flan-t5-large', \n",
    "#     model_kwargs={'temperature':0.6, 'max_length':64}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me the capital of Bangladesh'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an object of the PromptTemplate class\n",
    "prompt_template_01=PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template='Tell me the capital of {country}',\n",
    ")\n",
    "prompt_template_01.format(country='Bangladesh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing LLMChain from langchain.chains and creating an object of LLMChain()\n",
    "from langchain.chains import LLMChain\n",
    "chain=LLMChain(                     \n",
    "    llm=llm,    # we can use either OpenAI-API-KEY or HuggingFace-API_TOKEN to get the answer.\n",
    "    prompt=prompt_template_01,\n",
    "\n",
    ")\n",
    "# chain.run('Bangladesh')\n",
    "\n",
    "# print the output\n",
    "print(chain.run('Bangladesh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining Multiple Chains Using Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating multiple chains through PromtTemplate() class\n",
    "capital_template=PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template='Please tell me the capital of {country}'\n",
    "    )\n",
    "capital_chain=LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=capital_template\n",
    ")\n",
    "\n",
    "# this is another chain\n",
    "famous_template=PromptTemplate(\n",
    "    input_variables=['capital'],\n",
    "    template='Suggest me some amazing places to visit in {capital}'\n",
    "    )\n",
    "famous_chain=LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=famous_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to combine or bind those chains like capital_chain and famous_chain \n",
    "# into SimpleSequentialChain for that we have to import the SimpleSequentialChain() class\n",
    "# from langchain.chains library\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
