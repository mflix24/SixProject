{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries from the langchain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing os and get some methods from it.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# os.environ['OPEN_API_KEY']= OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if openai_api_key:\n",
    "#     print(f\"Successfully loaded OPENAI_API_KEY: YOUR_OPENAI_API_KEY\")\n",
    "# else:\n",
    "#     print(\"Error: OPENAI_API_KEY not found in the .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temperature value: 0 means the model is very safe and it is not taking any bets. the more it tends to 1, it will be taking a lot more bets and risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the temperature parameter has important role like\n",
    "# its ranges from 0 to 1. The more tend to 1 means \n",
    "# that the different output we may have.\n",
    "llm=OpenAI(openai_api_key=openai_api_key, temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my question to be known\n",
    "text=\"What is the capital of Bangladesh?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of Bangladesh is Dhaka.\n"
     ]
    }
   ],
   "source": [
    "# print the result\n",
    "# llm.predict(text)\n",
    "print(llm.predict(text))\n",
    "\n",
    "#################3 this is end of using OPENAI-API-KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using flan-t5-large model by HuggingFace Token on the top of Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers python-dotenv        flan-t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# importing\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Access the Hugging Face API token\n",
    "hf_api_token = os.getenv(\"HF_HOME_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing HuggingFaceHub() from langchain and creating an object of using it\n",
    "from langchain import HuggingFaceHub\n",
    "llms_huggingfacehub=HuggingFaceHub(\n",
    "    huggingfacehub_api_token=hf_api_token, \n",
    "    repo_id='google/flan-t5-large', \n",
    "    model_kwargs={'temperature':0.6, 'max_length':64}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dhaka'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outout or prediction\n",
    "llms_huggingfacehub.predict(\"what is the capital of Bangladesh?\")\n",
    "# this is how we can use open source mode here we used \n",
    "# flan-t5-large by HuggingFace API Tokens on the top of Langchain.\n",
    "\n",
    "\n",
    "############ this is end of using HuggingFace API Tokens for using the open source model as 'flan-t5-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # these code is generated from chat.openai.com\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "# # Access the Hugging Face API token\n",
    "# hf_api_token = os.getenv(\"HF_HOME_TOKEN\")\n",
    "# if not hf_api_token:\n",
    "#     raise ValueError(\"Hugging Face API token not found in the .env file.\")\n",
    "\n",
    "# # Set the API token for Hugging Face authentication\n",
    "# os.environ[\"HF_HOME_TOKEN\"] = hf_api_token\n",
    "\n",
    "# # Replace 'flan-t5-large' with the actual model identifier\n",
    "# model_name = \"google/flan-t5-large\"\n",
    "\n",
    "# # Load the tokenizer and model  ########here the error occured #####\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# # Example input text\n",
    "# input_text = \"Translate this text to another language.\"\n",
    "\n",
    "# # Tokenize the input text\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# # Generate output from the model\n",
    "# output_ids = model.generate(input_ids)\n",
    "\n",
    "# # Decode the output tokens to get the translated text\n",
    "# output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# # Print the result\n",
    "# print(f\"Input Text: {input_text}\")\n",
    "# print(f\"Translated Text: {output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompt Templates:\n",
    "- When we call LLM model then the LLM model should know what kind of \n",
    "- input it is probably expecting from the client or from the end user \n",
    "- and what kind of output it should give back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PromptTemplate() class from langchain.prompts\n",
    "# and creating an object of the PromptTemplate class\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the object llms_huggingfacehub\n",
    "# from langchain import HuggingFaceHub\n",
    "# llms_huggingfacehub=HuggingFaceHub(\n",
    "#     huggingfacehub_api_token=hf_api_token, \n",
    "#     repo_id='google/flan-t5-large', \n",
    "#     model_kwargs={'temperature':0.6, 'max_length':64}\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me the capital of Bangladesh'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating an object of the PromptTemplate class\n",
    "prompt_template_01=PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template='Tell me the capital of {country}',\n",
    ")\n",
    "prompt_template_01.format(country='Bangladesh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing LLMChain from langchain.chains and creating an object of LLMChain()\n",
    "from langchain.chains import LLMChain\n",
    "chain=LLMChain(                     \n",
    "    llm=llm,    # we can use either OpenAI-API-KEY or HuggingFace-API_TOKEN to get the answer.\n",
    "    prompt=prompt_template_01,\n",
    "\n",
    ")\n",
    "# chain.run('Bangladesh')\n",
    "\n",
    "# print the output\n",
    "print(chain.run('Bangladesh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining Multiple Chains Using Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating multiple chains through PromtTemplate() class\n",
    "capital_template=PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template='Please tell me the capital of {country}'\n",
    "    )\n",
    "capital_chain=LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=capital_template\n",
    ")\n",
    "\n",
    "# this is another chain\n",
    "famous_template=PromptTemplate(\n",
    "    input_variables=['capital'],\n",
    "    template='Suggest me some amazing places to visit in {capital}'\n",
    "    )\n",
    "famous_chain=LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=famous_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Here are some amazing places to visit in Dhaka:\\n\\n1. Lalbagh Fort - a 17th-century Mughal fort with beautiful gardens and architectural features.\\n2. Ahsan Manzil - also known as the Pink Palace, this stunning palace turned museum showcases the lifestyle of the Nawabs of Dhaka.\\n3. National Parliament House - a modern architectural marvel and the seat of the Parliament of Bangladesh.\\n4. Dhakeshwari Temple - an ancient Hindu temple dedicated to the Goddess Dhakeshwari.\\n5. Liberation War Museum - a must-visit for history buffs, this museum showcases the struggles and sacrifices of the Bangladeshi people during the Liberation War.\\n6. Sadarghat - the largest river port in Bangladesh, this bustling area is a great place to experience the daily life and culture of Dhaka.\\n7. National Museum of Bangladesh - the country's largest museum, with a vast collection of artifacts and exhibits showcasing the history and culture of Bangladesh.\\n8. Star Mosque - a beautiful mosque with intricate mosaic designs and star-shaped domes.\\n9. New Market - a popular shopping destination for traditional Bangladeshi handicrafts, clothes, and food.\\n10. Dhaka Zoo - home to a variety of animals and a great\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have to combine or bind those chains like capital_chain and famous_chain \n",
    "# into SimpleSequentialChain for that we have to import the SimpleSequentialChain() class\n",
    "# from langchain.chains library\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "chain=SimpleSequentialChain(\n",
    "    chains=[capital_chain,famous_chain] \n",
    ")\n",
    "chain.run('Bangladesh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### usage of SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating multiple chains through PromtTemplate() class\n",
    "capital_template=PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template='Please tell me the capital of {country}'\n",
    "    )\n",
    "capital_chain=LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=capital_template,\n",
    "    output_key='capital'\n",
    ")\n",
    "\n",
    "# this is another chain\n",
    "famous_template=PromptTemplate(\n",
    "    input_variables=['capital'],\n",
    "    template='Suggest me some amazing places to visit in {capital}'\n",
    "    )\n",
    "famous_chain=LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=famous_template,\n",
    "    output_key='places'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing SequentialChain() class from langchain.chains\n",
    "from langchain.chains import SequentialChain\n",
    "chain=SequentialChain(\n",
    "    chains=[capital_chain,famous_chain],\n",
    "    input_variables=['country'],\n",
    "    output_variables=['capital','places']\n",
    ")\n",
    "chain({'country':'Bangladesh'})\n",
    "# for printing\n",
    "# print(chain({'country':'Bangladesh'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversational ChatBot with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing ChatOpenAI() class from langchain.chat_models \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# ChatOpenAI() class has 3 schemas such as HumanMessage, SystemMessage and AIMessage.\n",
    "# importing them through langchain.schema\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is my ChatOpenAI() model using openai_api_key and using gpt-3.5-turbo\n",
    "chatllm=ChatOpenAI(\n",
    "    openai_api_key=openai_api_key, \n",
    "    temperature=0.6,\n",
    "    model='gpt-3.5-turbo'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"In the heart of a bustling city's throng,\\nMankind walks the earth, both weak and strong.\\nWith dreams that reach beyond the stars,\\nAnd scars that tell of battles fought in wars.\\n\\nThrough history's pages, their story told,\\nOf triumphs grand and hearts grown cold.\\nThey strive for love, for peace, for truth,\\nYet often wander in the dark, uncouth.\\n\\nThey build great towers, touch the sky,\\nYet often fail to heed the silent cry\\nOf nature's plea, of harmony lost,\\nOf bridges burned and lines uncrossed.\\n\\nBut in the depths of every soul,\\nThere lies a spark, a sacred goal.\\nTo rise above, to learn, to grow,\\nTo nurture kindness, let compassion flow.\\n\\nSo let us cherish mankind's worth,\\nAnd celebrate their journey on this earth.\\nFor in their flaws and beauty intertwined,\\nA tapestry of hope and love, defined.\")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling the schema of SystemMessage and HumanMessage\n",
    "chatllm([\n",
    "    SystemMessage(content='You are a poet AI assistant'),\n",
    "    HumanMessage(content='Please provide some poem about Mankind')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ChatPromptTemplate + LLM + Output Parsers\n",
    "\n",
    "-- User\n",
    "what is output parser with respect to AI\n",
    "\n",
    "ChatGPT\n",
    "In the context of AI, an output parser refers to a component or module that is responsible for analyzing and extracting relevant information from the output generated by an AI system. The output parser plays a crucial role in converting raw output into a structured and usable format.\n",
    "\n",
    "Here's how an output parser typically works:\n",
    "\n",
    "Raw Output:\n",
    "After processing input data, an AI system produces output. This output can be in the form of raw text, numerical values, or other types of data depending on the nature of the task the AI is designed for.\n",
    "\n",
    "Parsing:\n",
    "The output parser takes this raw output and parses it, which means it interprets and analyzes the data to identify specific elements or pieces of information. Parsing involves breaking down the output into its constituent parts.\n",
    "\n",
    "Extraction:\n",
    "Once parsed, the output parser extracts the relevant information from the output. This could involve identifying key entities, relationships, or patterns within the data.\n",
    "\n",
    "Structured Representation:\n",
    "The extracted information is then organized and represented in a structured format that is more suitable for further processing or presentation. This structured representation makes it easier for downstream applications or users to make use of the output.\n",
    "\n",
    "Output parsers are commonly used in various AI applications, including natural language processing (NLP), computer vision, and other domains where AI systems generate output that needs to be understood and utilized. By employing output parsers, AI systems can enhance the interpretability and usability of their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a class for parser the output data.\n",
    "class CommaSeparatedOutput(BaseOutputParser):\n",
    "    def parse(self,text:str):\n",
    "        return text.strip().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating template for SystemMessage schema\n",
    "system_template='You are a helpful assistant.You should give me 5 synonims words seperating by comma when an user gives any input'\n",
    "human_template='{text}'\n",
    "chat_prompt_template=ChatPromptTemplate.from_messages([\n",
    "    ('system',system_template),\n",
    "    ('human',human_template)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is my ChatOpenAI() model using openai_api_key and using gpt-3.5-turbo\n",
    "chat_llm=ChatOpenAI(\n",
    "    openai_api_key=openai_api_key, \n",
    "    temperature=0.6,\n",
    "    model='gpt-3.5-turbo'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaining All the 3 things like \n",
    "# ChatPromptTemplate + ChatLLM + Output Parser\n",
    "chain=chat_prompt_template|chat_llm|CommaSeparatedOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Excellent', ' great', ' fine', ' superb', ' wonderful']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AIMessage as output\n",
    "chain.invoke({\n",
    "    'text':'good'\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
